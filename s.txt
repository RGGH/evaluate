=== ./src/banner.rs ===
// src/banner.rs

/// Prints the application startup banner to the console.
pub fn print_banner() {
    // Using a raw string literal for the multi-line banner
    let banner = r#"
                   _                          
                  | |               _         
 _____ _   _ _____| | _   _ _____ _| |_ _____ 
| ___ | | | (____ | || | | (____ (_   _) ___ |
| ____|\ V // ___ | || |_| / ___ | | |_| ____|
|_____) \_/ \_____|\_)____/\_____|  \__)_____)
                                                                                                                                                                                                       

    LLM Evaluation & Testing Framework
"#;
    println!("{}", banner);
}
=== ./src/providers/ollama.rs ===
// src/providers/ollama.rs

use async_trait::async_trait;
use reqwest::Client;
use serde::{Deserialize, Serialize};
use std::time::Instant;

use crate::config::OllamaConfig;
use crate::errors::{EvalError, Result};
use crate::providers::LlmProvider;

/// A provider for interacting with local Ollama models.
pub struct OllamaProvider {
    client: Client,
    config: OllamaConfig,
}

#[derive(Serialize)]
struct OllamaRequest<'a> {
    model: &'a str,
    prompt: &'a str,
    stream: bool,
}

#[derive(Deserialize)]
struct OllamaResponse {
    response: String,
}

impl OllamaProvider {
    /// Creates a new `OllamaProvider`.
    pub fn new(client: Client, config: OllamaConfig) -> Self {
        Self { client, config }
    }
}

#[async_trait]
impl LlmProvider for OllamaProvider {
    /// Calls the Ollama API with a given prompt and returns the model's response text and latency.
    async fn generate(&self, model: &str, prompt: &str) -> Result<(String, u64)> {
        let url = format!("{}/api/generate", self.config.api_base.trim_end_matches('/'));

        println!("📡 Calling Ollama: {} with model: {}", url, model);

        let body = OllamaRequest {
            model,
            prompt,
            stream: false,
        };

        let start = Instant::now();

        let resp = self.client.post(&url).json(&body).send().await?;

        let status = resp.status();
        let latency_ms = start.elapsed().as_millis() as u64;

        println!("📥 Ollama response status: {} ({}ms)", status, latency_ms);

        if !status.is_success() {
            let error_body = resp
                .text()
                .await
                .unwrap_or_else(|_| "Could not read error body".to_string());
            return Err(EvalError::ApiError {
                status: status.as_u16(),
                body: error_body,
            });
        }

        let ollama_resp: OllamaResponse = resp.json().await?;
        if ollama_resp.response.is_empty() {
            return Err(EvalError::EmptyResponse);
        }

        Ok((ollama_resp.response, latency_ms))
    }
}
=== ./src/providers/gemini.rs ===
// src/providers/gemini.rs

use async_trait::async_trait;
use reqwest::Client;
use serde_json::json;
use std::time::Instant;

use crate::config::GeminiConfig;
use crate::errors::{EvalError, Result};
use crate::providers::LlmProvider;

/// A provider for interacting with Google's Gemini models.
pub struct GeminiProvider {
    client: Client,
    config: GeminiConfig,
}

impl GeminiProvider {
    /// Creates a new `GeminiProvider`.
    pub fn new(client: Client, config: GeminiConfig) -> Self {
        Self { client, config }
    }
}

#[async_trait]
impl LlmProvider for GeminiProvider {
    /// Calls the Gemini API with a given prompt and returns the model's response text and latency.
    async fn generate(&self, model: &str, prompt: &str) -> Result<(String, u64)> {
        let url = format!(
            "{}/v1beta/models/{}:generateContent",
            self.config.api_base.trim_end_matches('/'),
            model
        );

        println!("📡 Calling Gemini: {} with model: {}", url, model);

        let body = json!({
            "contents": [{"parts": [{"text": prompt}]}]
        });

        let start = Instant::now();

        let resp = self
            .client
            .post(&url)
            .header("x-goog-api-key", &self.config.api_key)
            .json(&body)
            .send()
            .await?;

        let status = resp.status();
        let latency_ms = start.elapsed().as_millis() as u64;

        println!("📥 Gemini response status: {} ({}ms)", status, latency_ms);

        if !status.is_success() {
            let error_body = resp
                .text()
                .await
                .unwrap_or_else(|_| "Could not read error body".to_string());
            return Err(EvalError::ApiError {
                status: status.as_u16(),
                body: error_body,
            });
        }

        let response_json: serde_json::Value = resp.json().await?;

        if let Some(error) = response_json.get("error") {
            return Err(EvalError::ApiResponse(error.to_string()));
        }

        let output = response_json
            .get("candidates")
            .and_then(|c| c.get(0))
            .and_then(|c| c.get("content"))
            .and_then(|c| c.get("parts"))
            .and_then(|p| p.get(0))
            .and_then(|p| p.get("text"))
            .and_then(|t| t.as_str())
            .ok_or_else(|| EvalError::UnexpectedResponse(response_json.to_string()))?;

        if output.is_empty() {
            return Err(EvalError::EmptyResponse);
        }

        Ok((output.to_string(), latency_ms))
    }
}
=== ./src/providers/mod.rs ===
// src/providers/mod.rs

use async_trait::async_trait;
use crate::errors::Result;
use std::sync::Arc;

pub mod gemini;
pub mod ollama;

/// A common trait for Large Language Model (LLM) providers.
/// This allows for a unified interface to different model backends like Gemini, Ollama, etc.
#[async_trait]
pub trait LlmProvider: Send + Sync {
    /// Generates a response from the LLM based on a given prompt.
    ///
    /// # Arguments
    /// * `model` - The specific model to use for generation (e.g., "gemini-1.5-flash-latest").
    /// * `prompt` - The input prompt to send to the model.
    ///
    /// # Returns
    /// A `Result` containing a tuple of the generated `String` and the latency in milliseconds (`u64`).
    async fn generate(&self, model: &str, prompt: &str) -> Result<(String, u64)>;
}

/// A type alias for a thread-safe, shareable LlmProvider.
pub type Provider = Arc<dyn LlmProvider>;
=== ./src/database.rs ===
// src/database.rs - Fixed directory creation
use crate::models::{ApiResponse, EvalResult};
use sqlx::{sqlite::SqlitePoolOptions, Row, SqlitePool};
use std::path::PathBuf;

pub async fn init_db() -> Result<SqlitePool, sqlx::Error> {
    let db_path = get_db_path()?;
    
    // Create parent directory BEFORE attempting to connect
    if let Some(parent) = db_path.parent() {
        std::fs::create_dir_all(parent)
            .map_err(sqlx::Error::Io)?;
        println!("✅ Created database directory: {}", parent.display());
    }
    
    // Ensure the path is absolute and properly formatted
    let absolute_path = if db_path.is_relative() {
        std::env::current_dir()
            .map_err(sqlx::Error::Io)?
            .join(&db_path)
    } else {
        db_path.clone()
    };
    
    println!("📦 Database file path: {}", absolute_path.display());
    
    // SQLite connection string needs to be properly formatted
    let db_url = format!("sqlite://{}?mode=rwc", absolute_path.display());
    println!("📦 Connecting to: {}", db_url);

    let pool = SqlitePoolOptions::new()
        .max_connections(5)
        .connect(&db_url)
        .await?;

    println!("✅ Database connected successfully");

    // Run migrations from the migrations directory
    sqlx::migrate!("./migrations")
        .run(&pool)
        .await?;

    println!("✅ Database migrations completed");

    Ok(pool)
}

fn get_db_path() -> Result<PathBuf, sqlx::Error> {
    let db_url = std::env::var("DATABASE_URL")
        .map_err(|_| sqlx::Error::Configuration("DATABASE_URL must be set".into()))?;
    
    let db_path_str = db_url.strip_prefix("sqlite:").ok_or_else(|| {
        sqlx::Error::Configuration("DATABASE_URL must start with 'sqlite:'".into())
    })?;
    
    Ok(PathBuf::from(db_path_str))
}

pub async fn save_evaluation(pool: &SqlitePool, response: &ApiResponse) -> Result<(), sqlx::Error> {
    let id = &response.id;
    let status = response.status.to_string();

    let (
        model,
        prompt,
        model_output,
        expected,
        judge_model,
        judge_verdict,
        judge_reasoning,
        error_message,
        created_at,
    ) = match &response.result {
            EvalResult::Success(res) => (
                Some(res.model.clone()),
                Some(res.prompt.clone()),
                Some(res.model_output.clone()),
                res.expected.clone(),
                res.judge_result.as_ref().map(|j| j.judge_model.clone()),
                res.judge_result.as_ref().map(|j| j.verdict.to_string()),
                res.judge_result.as_ref().map(|j| j.reasoning.clone()),
                None,
                Some(res.timestamp.clone()),
            ),
            EvalResult::Error(err) => (
                None,
                None,
                None,
                None,
                None,
                None,
                None,
                Some(err.message.clone()),
                None,
            ),
        };

    let created_at_str = created_at.unwrap_or_else(|| chrono::Utc::now().to_rfc3339());

    sqlx::query(
        r#"
        INSERT INTO evaluations (id, status, model, prompt, model_output, expected, judge_model, judge_verdict, judge_reasoning, error_message, created_at)
        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        "#
    )
    .bind(id)
    .bind(&status)
    .bind(&model)
    .bind(&prompt)
    .bind(&model_output)
    .bind(&expected)
    .bind(&judge_model)
    .bind(&judge_verdict)
    .bind(&judge_reasoning)
    .bind(&error_message)
    .bind(&created_at_str)
    .execute(pool)
    .await?;

    Ok(())
}

pub async fn get_all_evaluations(pool: &SqlitePool) -> Result<Vec<HistoryEntry>, sqlx::Error> {
    let rows = sqlx::query(
        r#"
        SELECT id, status, model, prompt, model_output, expected, judge_model, judge_verdict, judge_reasoning, error_message, created_at
        FROM evaluations
        ORDER BY created_at DESC
        "#
    )
    .fetch_all(pool)
    .await?;

    Ok(rows.into_iter().map(|row| HistoryEntry {
        id: row.get(0),
        status: row.get(1),
        model: row.get(2),
        prompt: row.get(3),
        model_output: row.get(4),
        expected: row.get(5),
        judge_model: row.get(6),
        judge_verdict: row.get(7),
        judge_reasoning: row.get(8),
        error_message: row.get(9),
        created_at: row.get(10),
    }).collect())
}

#[derive(serde::Serialize, Clone)]
pub struct HistoryEntry {
    pub id: String,
    pub status: Option<String>,
    pub model: Option<String>,
    pub prompt: Option<String>,
    pub model_output: Option<String>,
    pub expected: Option<String>,
    pub judge_model: Option<String>,
    pub judge_verdict: Option<String>,
    pub judge_reasoning: Option<String>,
    pub error_message: Option<String>,
    pub created_at: String,
}
=== ./src/api.rs ===
// src/api.rs
use actix_web::{web, HttpResponse, Responder};
use std::collections::HashMap;
use std::sync::Arc;

use crate::config::{AppConfig, EvalConfig};
use crate::database;
use crate::errors::Result;
use crate::providers::{
    gemini::GeminiProvider,
    ollama::OllamaProvider,
    Provider,
};
use crate::runner::{run_batch_evals, run_eval};

#[derive(Clone)]
pub struct AppState {
    pub providers: HashMap<String, Provider>,
    pub db_pool: sqlx::SqlitePool,
    pub http_client: reqwest::Client,
}

impl AppState {
    pub async fn new(config: AppConfig) -> Self {
        let db_pool = database::init_db().await.expect("Database initialization failed");
        let http_client = reqwest::Client::new();
        let mut providers = HashMap::new();

        if let Some(gemini_config) = config.gemini {
            println!("✅ Gemini provider configured.");
            let gemini_provider = Arc::new(GeminiProvider::new(http_client.clone(), gemini_config));
            providers.insert("gemini".to_string(), gemini_provider);
        }

        if let Some(ollama_config) = config.ollama {
            println!("✅ Ollama provider configured.");
            let ollama_provider = Arc::new(OllamaProvider::new(http_client.clone(), ollama_config));
            providers.insert("ollama".to_string(), ollama_provider);
        }

        Self {
            providers,
            db_pool,
            http_client,
        }
    }
}

async fn run_single_eval(
    state: web::Data<AppState>,
    eval_config: web::Json<EvalConfig>,
) -> impl Responder {
    "Not implemented yet"
}

pub fn configure_routes(cfg: &mut web::ServiceConfig) {
    cfg.service(web::resource("/v1/evals/run").route(web::post().to(run_single_eval)));
}
=== ./src/runner.rs ===
// src/runner.rs
use crate::api::AppState;
use crate::config::EvalConfig;
use crate::errors::{EvalError, Result};
use crate::providers::Provider;
use futures::future;
use serde::{Deserialize, Serialize};
use std::time::Instant;

#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct EvalResult {
    pub model: String,
    pub prompt: String,
    pub model_output: String,
    pub expected: Option<String>,
    pub judge_result: Option<JudgeResult>,
    pub timestamp: String,
    pub latency_ms: u64,
    pub judge_latency_ms: Option<u64>,
    pub total_latency_ms: u64,
}

#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct JudgeResult {
    pub judge_model: String,
    pub verdict: JudgeVerdict,
    pub reasoning: Option<String>,
    pub confidence: Option<f32>,
}

#[derive(Debug, Serialize, Deserialize, Clone)]
pub enum JudgeVerdict {
    Pass,
    Fail,
    Uncertain,
}

impl std::fmt::Display for JudgeVerdict {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        match self {
            JudgeVerdict::Pass => write!(f, "Pass"),
            JudgeVerdict::Fail => write!(f, "Fail"),
            JudgeVerdict::Uncertain => write!(f, "Uncertain"),
        }
    }
}

/// Parse judge response to extract verdict and reasoning
fn parse_judge_response(response: &str) -> JudgeResult {
    let response_lower = response.to_lowercase();
    
    // Try to extract structured response if available
    let verdict = if response_lower.contains("verdict: pass") || 
                     (response_lower.starts_with("yes") || response_lower.contains("yes, they")) {
        JudgeVerdict::Pass
    } else if response_lower.contains("verdict: fail") || 
              (response_lower.starts_with("no") || response_lower.contains("no, they")) {
        JudgeVerdict::Fail
    } else {
        JudgeVerdict::Uncertain
    };

    // Extract reasoning if present (look for lines after the verdict)
    let reasoning = if response.len() > 20 {
        Some(response.to_string())
    } else {
        None
    };

    JudgeResult {
        judge_model: "unknown".to_string(),
        verdict,
        reasoning,
        confidence: None,
    }
}

/// Enhanced judge prompt with better structure
fn create_judge_prompt(expected: &str, actual: &str, criteria: Option<&str>) -> String {
    let base_criteria = criteria.unwrap_or(
        "The outputs should convey the same core meaning, even if phrased differently."
    );

    format!(
        r#"You are an expert evaluator comparing two text outputs.

EVALUATION CRITERIA:
{}

EXPECTED OUTPUT:
{}

ACTUAL OUTPUT:
{}

INSTRUCTIONS:
1. Carefully compare both outputs
2. Consider semantic equivalence, not just exact wording
3. Provide your verdict as the first line: "Verdict: PASS" or "Verdict: FAIL"
4. Then explain your reasoning in 2-3 sentences

Your evaluation:"#,
        base_criteria,
        expected,
        actual
    )
}

/// Parses a model string like "provider:model_name" and returns the provider and model.
/// Defaults to "gemini" if no provider is specified.
fn parse_model_string(model_str: &str) -> (String, String) {
    match model_str.split_once(':') {
        Some((provider, model)) => (provider.to_string(), model.to_string()),
        None => ("gemini".to_string(), model_str.to_string()),
    }
}

/// Selects the appropriate provider from the AppState based on the provider name.
fn get_provider(state: &AppState, provider_name: &str) -> Result<Provider> {
    state
        .providers
        .get(provider_name)
        .cloned()
        .ok_or_else(|| EvalError::ProviderNotFound(provider_name.to_string()))
}

/// Run a single eval with comprehensive LLM-as-a-judge evaluation
pub async fn run_eval(
    state: &AppState, 
    eval: &EvalConfig, 
) -> Result<EvalResult> {
    // Step 0: Render the eval config to substitute variables from metadata
    let rendered_eval = eval.render()?;

    let eval_start = Instant::now();
    let separator = "=".repeat(60);
    println!("\n{}", separator);
    println!("🎯 Starting evaluation for model: {}", rendered_eval.model);
    println!("{}\n", separator);

    // Step 1: Call the target model
    let (provider_name, model_name) = parse_model_string(&rendered_eval.model);
    let provider = get_provider(state, &provider_name)?;

    println!("📝 Prompt: {}", rendered_eval.prompt);
    
    let (model_output_str, latency_ms) = provider
    .generate(&model_name, &rendered_eval.prompt)
    .await
    .map_err(|e| {
        eprintln!("❌ Model failed: {}", e);
        EvalError::ModelFailure {
            model: rendered_eval.model.clone(),
        }
    })?;

    println!("\n✅ Model Output ({}ms):\n{}\n", latency_ms, model_output_str);

    // Step 2: Run judge evaluation if expected output provided
    let mut judge_latency_ms = None;
    let judge_result = if let (Some(expected), Some(judge_model)) =
        (&rendered_eval.expected, &rendered_eval.judge_model) {
        
        println!("⚖️  Running judge evaluation with model: {}", judge_model);
        
        let judge_prompt = create_judge_prompt(
            expected,
            &model_output_str,
            rendered_eval.criteria.as_deref()
        );

        let (judge_provider_name, judge_model_name) = parse_model_string(judge_model);
        let judge_provider = get_provider(state, &judge_provider_name)?;

        match judge_provider.generate(&judge_model_name, &judge_prompt).await
        {
            Ok((judge_response, judge_latency)) => {
                judge_latency_ms = Some(judge_latency);
                println!("\n⚖️  Judge Response ({}ms):\n{}\n", judge_latency, judge_response);
                
                let mut result = parse_judge_response(&judge_response);
                result.judge_model = judge_model.clone();
                
                match result.verdict {
                    JudgeVerdict::Pass => println!("✅ VERDICT: PASS"),
                    JudgeVerdict::Fail => println!("❌ VERDICT: FAIL"),
                    JudgeVerdict::Uncertain => println!("⚠️  VERDICT: UNCERTAIN"),
                }
                
                Some(result)
            }
            Err(e) => {
                let judge_error = EvalError::JudgeFailure {
                    model: judge_model.clone(),
                    source: Box::new(e),
                };
                eprintln!("⚠️  Judge evaluation failed: {}", judge_error);
                None
            }
        }
    } else {
        println!("ℹ️  No judge evaluation (no expected output or judge model specified)");
        None
    };

    let total_latency_ms = eval_start.elapsed().as_millis() as u64;
    println!("⏱️  Total evaluation time: {}ms", total_latency_ms);
    println!("\n{}\n", separator);

    Ok(EvalResult {
        model: rendered_eval.model.clone(),
        prompt: rendered_eval.prompt.clone(),
        model_output: model_output_str,
        expected: rendered_eval.expected.clone(),
        judge_result,
        timestamp: chrono::Utc::now().to_rfc3339(),
        latency_ms,
        judge_latency_ms,
        total_latency_ms,
    })
}

/// Run multiple evals and aggregate results concurrently
pub async fn run_batch_evals(
    state: &AppState,
    evals: Vec<EvalConfig>,
) -> Vec<Result<EvalResult>> {
    let batch_start = Instant::now();
    let total_evals = evals.len();

    // list of futures and run them concurrently within the same task **
    let futures: Vec<_> = evals
        .iter()
        .map(|eval| run_eval(state, eval))
        .collect();

    let results = future::join_all(futures).await;

    let batch_total_ms = batch_start.elapsed().as_millis() as u64;
    println!("\n📊 Batch of {} completed concurrently in {}ms", total_evals, batch_total_ms);

    results
}
=== ./src/errors.rs ===
// src/errors.rs
use thiserror::Error;

#[derive(Error, Debug)]
#[allow(dead_code)]
pub enum EvalError {
    #[error("Failed to read file: {0}")]
    FileRead(#[from] std::io::Error),

    #[error("Failed to parse TOML config: {0}")]
    TomlParse(#[from] toml::de::Error),

    #[error("Failed to parse JSON config: {0}")]
    JsonParse(#[from] serde_json::Error),

    #[error("HTTP request failed: {0}")]
    Request(#[from] reqwest::Error),

    #[error("API request failed with status {status}: {body}")]
    ApiError { status: u16, body: String },

    #[error("API returned an error: {0}")]
    ApiResponse(String),

    #[error("Unexpected response structure: {0}")]
    UnexpectedResponse(String),

    #[error("Received empty text response from model")]
    EmptyResponse,

    #[error("Model '{model}' failed to respond")]
    ModelFailure { model: String },

    #[error("Judge model '{model}' failed: {source}")]
    JudgeFailure {
        model: String,
        #[source]
        source: Box<EvalError>,
    },
}

pub type Result<T> = std::result::Result<T, EvalError>;

=== ./src/main.rs ===

pub mod config;
mod errors;
mod providers;
mod runner;
mod models;
mod database;
mod banner;
 
use actix_web::{web, App, HttpRequest, HttpResponse, HttpServer, middleware, Responder, error::Error};
use actix_cors::Cors;
use api::{configure_routes, AppState};
use rust_embed::RustEmbed;
use std::borrow::Cow;

#[derive(RustEmbed)]
#[folder = "static/"]
struct StaticAssets;

#[actix_web::main]
async fn main() -> std::io::Result<()> {
    // Print the startup banner
    banner::print_banner();

    // Load .env file - fail loudly if it doesn't exist
    if let Err(e) = dotenv::dotenv() {
        eprintln!("⚠️  Warning: Could not load .env file: {}", e);
        eprintln!("   Make sure DATABASE_URL is set in your environment");
    }
    
    // Debug: Check if DATABASE_URL is set
    match std::env::var("DATABASE_URL") {
        Ok(url) => println!("✅ DATABASE_URL set to: {}", url),
        Err(_) => eprintln!("❌ DATABASE_URL not set!"),
    }
    
    env_logger::init_from_env(env_logger::Env::new().default_filter_or("info"));
    
    let app_config = config::AppConfig::from_env()
        .expect("Failed to load app configuration from environment");
    
    let state = AppState::new(app_config).await;
    
    println!("🚀 Starting server...");
    println!("📊 Frontend available at http://127.0.0.1:8080");

    HttpServer::new(move || {
        let cors = Cors::permissive();
        
        App::new()
            .app_data(actix_web::web::Data::new(state.clone()))
            .wrap(cors)
            .wrap(middleware::Logger::default())
            .configure(configure_routes)
            .route("/{_:.*}", web::get().to(static_file_handler))
    })
    .bind(("0.0.0.0", 8080))?
    .run()
    .await
}

async fn static_file_handler(req: HttpRequest) -> impl Responder {
    let path = if req.path() == "/" {
        "index.html"
    } else {
        // trim leading '/'
        &req.path()[1..]
    };

    match StaticAssets::get(path) {
        Some(content) => {
            let mime = mime_guess::from_path(path).first_or_octet_stream();
            HttpResponse::Ok().content_type(mime.as_ref()).body(Cow::into_owned(content.data))
        }
        None => HttpResponse::NotFound().body("404 Not Found"),
    }
}
=== ./src/config.rs ===
// src/config.rs
use serde::Deserialize;
use regex::Regex;
use crate::errors::{Result, EvalError};

/// Configuration for the Gemini provider.
#[derive(Debug, Clone)]
pub struct GeminiConfig {
    pub api_base: String,
    pub api_key: String,
    pub models: Vec<String>,
}

/// Configuration for the Ollama provider.
#[derive(Debug, Clone)]
pub struct OllamaConfig {
    pub api_base: String,
    pub models: Vec<String>,
}

/// High-level application configuration loaded from environment variables.
#[derive(Debug, Clone)]
pub struct AppConfig {
    pub gemini: Option<GeminiConfig>,
    pub ollama: Option<OllamaConfig>,
}

/// Contains all the information needed to run one prompt against a model
/// The model string is expected to be in the format `provider:model_name`,
/// e.g., `gemini:gemini-1.5-flash` or `ollama:llama3`.
/// If no provider is specified, it will default to `gemini`.
#[derive(Deserialize, Debug, Clone)]
pub struct EvalConfig {
    /// The model to evaluate
    pub model: String,
    
    /// The prompt to send to the model
    pub prompt: String,
    
    /// Expected output for comparison (optional)
    #[serde(default)]
    pub expected: Option<String>,
    
    /// Judge model for LLM-as-a-judge evaluation (optional)
    #[serde(default)]
    pub judge_model: Option<String>,
    
    /// Custom evaluation criteria (optional)
    /// If not provided, default semantic equivalence criteria will be used
    #[serde(default)]
    pub criteria: Option<String>,
    
    /// Tags for organizing evals
    #[serde(default)]
    pub tags: Vec<String>,
    
    /// Metadata for the eval
    #[serde(default)]
    pub metadata: Option<serde_json::Value>,
}

impl AppConfig {
    /// Load configuration from environment variables
    pub fn from_env() -> Result<Self> {
        let mut gemini_config = None;
        if let Ok(api_key) = std::env::var("GEMINI_API_KEY") {
            let api_base = std::env::var("GEMINI_API_BASE")
                .unwrap_or_else(|_| "https://generativelanguage.googleapis.com".to_string());
            let models_str = std::env::var("GEMINI_MODELS").unwrap_or_else(|_| {
                "gemini-1.5-pro-latest,gemini-1.5-flash-latest".to_string()
            });
            let models = models_str.split(',').map(|s| s.trim().to_string()).collect();
            gemini_config = Some(GeminiConfig { api_base, api_key, models });
        }

        let mut ollama_config = None;
        if let Ok(api_base) = std::env::var("OLLAMA_API_BASE") {
             let models_str = std::env::var("OLLAMA_MODELS").unwrap_or_else(|_| {
                "llama3,gemma".to_string()
            });
            let models = models_str.split(',').map(|s| s.trim().to_string()).collect();
            ollama_config = Some(OllamaConfig { api_base, models });
        }

        if gemini_config.is_none() && ollama_config.is_none() {
            return Err(EvalError::Config(
                "No LLM providers configured. Please set either GEMINI_API_KEY or OLLAMA_API_BASE.".to_string()
            ));
        }

        Ok(AppConfig { gemini: gemini_config, ollama: ollama_config })
    }
}

impl EvalConfig {


    /// Creates a new `EvalConfig` by substituting placeholders from its metadata.
    /// Placeholders are in the format `{{key}}`.
    pub fn render(&self) -> Result<Self> {
        let mut rendered_config = self.clone();

        if let Some(metadata) = &self.metadata {
            rendered_config.prompt = render_template(&self.prompt, metadata);
            if let Some(expected) = &self.expected {
                rendered_config.expected = Some(render_template(expected, metadata));
            }
        }

        Ok(rendered_config)
    }
}

/// Simple template renderer using regex.
fn render_template(template: &str, data: &serde_json::Value) -> String {
    let re = Regex::new(r"\{\{\s*(\w+)\s*\}\}").unwrap();
    re.replace_all(template, |caps: &regex::Captures| {
        let key = &caps[1];
        data.get(key)
            .and_then(|v| v.as_str())
            .map(|s| s.to_string())
            .unwrap_or_else(|| caps[0].to_string()) // Keep original placeholder if key not found
    }).to_string()
}

#[cfg(test)]
mod tests {
    use super::*;
    use serde_json::json;

    #[test]
    fn test_eval_config_render() {
        // 1. Create an EvalConfig with placeholders
        let eval_config = EvalConfig {
            model: "gemini-2.5-flash".to_string(),
            prompt: "What is the capital of {{country}}?".to_string(),
            expected: Some("The capital is {{capital}}.".to_string()),
            judge_model: Some("gemini-2.5-pro".to_string()),
            criteria: None,
            tags: vec!["geography".to_string()],
            // 2. Add metadata with the values to substitute
            metadata: Some(json!({
                "country": "France",
                "capital": "Paris"
            })),
        };

        // 3. Call the render function
        let rendered_config = eval_config.render().unwrap();

        // 4. Assert that the placeholders have been replaced
        assert_eq!(rendered_config.prompt, "What is the capital of France?");
        assert_eq!(
            rendered_config.expected,
            Some("The capital is Paris.".to_string())
        );

        // Metadata and other fields should remain unchanged
        assert_eq!(rendered_config.model, eval_config.model);
        assert_eq!(rendered_config.metadata, eval_config.metadata);
    }
}
=== ./src/models.rs ===
// src/models.rs
use crate::runner;
use serde::Serialize;

#[derive(Serialize, Clone)]
pub enum EvalResult {
    Success(runner::EvalResult),
    Error(ApiError),
}

#[derive(Serialize, Clone, Debug)]
pub struct ApiError {
    pub message: String,
}

#[derive(Serialize, Clone)]
pub struct ApiResponse {
    pub id: String,
    pub status: String,
    pub result: EvalResult,
}
=== ./src/api/handlers.rs ===
// src/api/handlers.rs
mod health;
mod evals;
mod experiments;
mod history;

pub use health::health_check;
pub use evals::{run_eval, run_batch, get_eval, get_status, get_history, get_models};
pub use experiments::{create_experiment, get_experiment};

=== ./src/api/handlers/history.rs ===
use actix_web::{web, HttpResponse, Result};
use serde::Serialize;
use crate::api::AppState;

#[derive(Serialize)]
pub struct HistoryResponse {
    pub results: Vec<crate::database::HistoryEntry>,
}

pub async fn get_history(
    state: web::Data<AppState>,
) -> Result<HttpResponse> {
    match state.db_pool.as_ref() {
        Some(pool) => {
            match crate::database::get_all_evaluations(pool).await {
                Ok(results) => Ok(HttpResponse::Ok().json(HistoryResponse { results })),
                Err(e) => {
                    eprintln!("Database error: {}", e);
                    Ok(HttpResponse::InternalServerError().json(serde_json::json!({
                        "error": "Failed to fetch history"
                    })))
                }
            }
        }
        None => Ok(HttpResponse::InternalServerError().json(serde_json::json!({
            "error": "Database not initialized"
        }))),
    }
}

=== ./src/api/handlers/health.rs ===
// src/api/handlers/health.rs
use actix_web::{HttpResponse, Result};
use serde_json::json;

pub async fn health_check() -> Result<HttpResponse> {
    Ok(HttpResponse::Ok().json(json!({
        "status": "healthy",
        "service": "eval-api",
        "version": env!("CARGO_PKG_VERSION")
    })))
}

=== ./src/api/handlers/experiments.rs ===
// src/api/handlers/experiments.rs
use actix_web::{web, HttpResponse, Result};
use serde::{Deserialize, Serialize};
use serde_json::json;
use uuid::Uuid;

#[derive(Deserialize)]
pub struct CreateExperimentRequest {
    pub name: String,
    pub description: Option<String>,
    pub eval_ids: Vec<String>,
}

#[derive(Serialize)]
pub struct ExperimentResponse {
    pub id: String,
    pub name: String,
    pub status: String,
    pub created_at: String,
}

pub async fn create_experiment(
    req: web::Json<CreateExperimentRequest>,
) -> Result<HttpResponse> {
    let experiment_id = Uuid::new_v4().to_string();
    
    Ok(HttpResponse::Created().json(ExperimentResponse {
        id: experiment_id,
        name: req.name.clone(),
        status: "created".to_string(),
        created_at: chrono::Utc::now().to_rfc3339(),
    }))
}

pub async fn get_experiment(path: web::Path<String>) -> Result<HttpResponse> {
    let experiment_id = path.into_inner();
    
    Ok(HttpResponse::Ok().json(json!({
        "id": experiment_id,
        "name": "Mock Experiment",
        "status": "completed",
        "results": {
            "total_evals": 10,
            "passed": 8,
            "failed": 2
        }
    })))
}

=== ./src/api/handlers/evals.rs ===
// src/api/handlers/evals.rs
use actix_web::{web, HttpResponse, Result};
use serde::{Deserialize, Serialize};
use uuid::Uuid;
use crate::api::AppState;
use crate::config::EvalConfig;
use crate::runner;
use serde_json::json;

#[derive(Clone, Deserialize)]
pub struct RunEvalRequest {
    pub model: String,
    pub prompt: String,
    pub expected: Option<String>,
    pub judge_model: Option<String>,
    pub criteria: Option<String>,
}

#[derive(Serialize)]
pub struct EvalResponse {
    pub id: String,
    pub status: String,
    pub result: Option<runner::EvalResult>,
    pub error: Option<String>,
}

#[derive(Serialize)]
pub struct BatchEvalResponse {
    pub batch_id: String,
    pub status: String,
    pub total: usize,
    pub completed: usize,
    pub passed: usize,
    pub failed: usize,
    pub average_model_latency_ms: u64,
    pub average_judge_latency_ms: u64,
    pub results: Vec<EvalResponse>,
}

pub async fn run_eval(
    state: web::Data<AppState>,
    req: web::Json<RunEvalRequest>,
) -> Result<HttpResponse> {
    let eval_id = Uuid::new_v4().to_string();
    let req_body = req.into_inner();
    let eval_config = EvalConfig {
        model: req_body.model,
        prompt: req_body.prompt,
        expected: req_body.expected,
        judge_model: req_body.judge_model,
        criteria: req_body.criteria,
        tags: Vec::new(), // Single run endpoint doesn't support tags
        metadata: None,   // Single run endpoint doesn't support metadata
    };

    match runner::run_eval(&state.config, &eval_config, &state.client).await {
        Ok(result) => {
            let status = if let Some(judge) = &result.judge_result {
                match judge.verdict {
                    runner::JudgeVerdict::Pass => "passed",
                    runner::JudgeVerdict::Fail => "failed",
                    runner::JudgeVerdict::Uncertain => "uncertain",
                }
            } else {
                "completed"
            };

            let response = EvalResponse {
                id: eval_id.clone(),
                status: status.to_string(),
                result: Some(result.clone()),
                error: None,
            };

            // Save to database
            if let Some(pool) = state.db_pool.as_ref() {
                let api_response = crate::models::ApiResponse {
                    id: eval_id,
                    status: status.to_string(),
                    result: crate::models::EvalResult::Success(result),
                };
                if let Err(e) = crate::database::save_evaluation(pool, &api_response).await {
                    log::error!("Failed to save evaluation to database: {}", e);
                }
            }

            Ok(HttpResponse::Ok().json(response))
        }
        Err(e) => {
            let error_string = e.to_string();
            Ok(HttpResponse::InternalServerError().json(EvalResponse {
                id: eval_id,
                status: "error".to_string(),
                result: None,
                error: Some(error_string),
            }))
        }
    }
}

pub async fn run_batch(
    state: web::Data<AppState>,
    eval_configs: web::Json<Vec<EvalConfig>>,
) -> Result<HttpResponse> {
    let batch_id = Uuid::new_v4().to_string();
    let total = eval_configs.len();

    let results = runner::run_batch_evals(
        &state.config,
        eval_configs.into_inner(),
        &state.client,
    ).await;

    let mut responses = Vec::new();
    let mut completed = 0;
    let mut passed = 0;
    let mut failed = 0;
    let mut total_model_latency = 0;
    let mut model_latency_count = 0;
    let mut total_judge_latency = 0;
    let mut judge_latency_count = 0;

    for result in results {
        let eval_id = Uuid::new_v4().to_string();
        
        match result {
            Ok(eval_result) => {
                completed += 1;
                total_model_latency += eval_result.latency_ms;
                model_latency_count += 1;
                if let Some(judge_latency) = eval_result.judge_latency_ms {
                    total_judge_latency += judge_latency;
                    judge_latency_count += 1;
                }
                
                let status = if let Some(judge) = &eval_result.judge_result {
                    match judge.verdict {
                        runner::JudgeVerdict::Pass => {
                            passed += 1;
                            "passed"
                        }
                        runner::JudgeVerdict::Fail => {
                            failed += 1;
                            "failed"
                        }
                        runner::JudgeVerdict::Uncertain => "uncertain",
                    }
                } else {
                    "completed"
                };

                let response = EvalResponse {
                    id: eval_id.clone(),
                    status: status.to_string(),
                    result: Some(eval_result.clone()),
                    error: None,
                };

                // Save to database
                if let Some(pool) = state.db_pool.as_ref() {
                    let api_response = crate::models::ApiResponse {
                        id: eval_id,
                        status: status.to_string(),
                        result: crate::models::EvalResult::Success(eval_result),
                    };
                    if let Err(e) = crate::database::save_evaluation(pool, &api_response).await {
                        log::error!("Failed to save batch evaluation to database: {}", e);
                    }
                }
                responses.push(EvalResponse {
                    ..response
                });
            }
            Err(e) => {
                failed += 1;
                responses.push(EvalResponse {
                    id: eval_id,
                    status: "error".to_string(),
                    result: None,
                    error: Some(e.to_string()),
                });
            }
        }
    }

    let average_model_latency_ms = if model_latency_count > 0 { total_model_latency / model_latency_count as u64 } else { 0 };
    let average_judge_latency_ms = if judge_latency_count > 0 { total_judge_latency / judge_latency_count as u64 } else { 0 };

    Ok(HttpResponse::Ok().json(BatchEvalResponse {
        batch_id,
        status: "completed".to_string(),
        total,
        completed,
        passed,
        failed,
        average_model_latency_ms,
        average_judge_latency_ms,
        results: responses,
    }))
}

pub async fn get_eval(path: web::Path<String>) -> Result<HttpResponse> {
    let eval_id = path.into_inner();
    
    Ok(HttpResponse::Ok().json(json!({
        "id": eval_id,
        "status": "completed",
        "message": "This endpoint would return stored eval results"
    })))
}

pub async fn get_status(path: web::Path<String>) -> Result<HttpResponse> {
    let eval_id = path.into_inner();
    
    Ok(HttpResponse::Ok().json(json!({
        "id": eval_id,
        "status": "completed",
        "progress": 100
    })))
}

#[derive(Serialize)]
pub struct HistoryResponse {
    pub results: Vec<crate::database::HistoryEntry>,
}

pub async fn get_history(state: web::Data<AppState>) -> Result<HttpResponse> {
    if let Some(pool) = state.db_pool.as_ref() {
        match crate::database::get_all_evaluations(pool).await {
            Ok(history) => Ok(HttpResponse::Ok().json(HistoryResponse { results: history })),
            Err(e) => {
                log::error!("Failed to fetch evaluation history: {}", e);
                Ok(HttpResponse::InternalServerError()
                    .json(json!({"error": "Failed to load history from database."})))
            }
        }
    } else {
        Ok(HttpResponse::Ok().json(HistoryResponse { results: vec![] }))
    }
}

#[derive(Serialize)]
pub struct ModelsResponse {
    pub models: Vec<String>,
}

pub async fn get_models(state: web::Data<AppState>) -> Result<HttpResponse> {
    Ok(HttpResponse::Ok().json(ModelsResponse { models: state.config.models.clone() }))
}
=== ./src/api/routes.rs ===
// src/api/routes.rs
use actix_web::web;
use super::handlers;

pub fn configure_routes(cfg: &mut web::ServiceConfig) {
    cfg.service(
        web::scope("/api/v1")
            .route("/health", web::get().to(handlers::health_check))
            .route("/models", web::get().to(handlers::get_models))
            .service(
                web::scope("/evals")
                    .route("/run", web::post().to(handlers::run_eval))
                    .route("/batch", web::post().to(handlers::run_batch))
                    .route("/history", web::get().to(handlers::get_history))
                    .route("/{id}", web::get().to(handlers::get_eval))
                    .route("/{id}/status", web::get().to(handlers::get_status))
            )
            .service(
                web::scope("/experiments")
                    .route("", web::post().to(handlers::create_experiment))
                    .route("/{id}", web::get().to(handlers::get_experiment))
            )
    );
}

=== ./src/api/state.rs ===
use crate::config::AppConfig;
use reqwest::Client;
use sqlx::SqlitePool;
use std::sync::Arc;

#[derive(Clone)]
pub struct AppState {
    pub config: Arc<AppConfig>,
    pub client: Client,
    pub db_pool: Arc<Option<SqlitePool>>,
}

impl AppState {
    pub async fn new(config: AppConfig) -> Self {
        let db_pool = crate::database::init_db()
            .await
            .ok();

        Self {
            config: Arc::new(config),
            client: Client::new(),
            db_pool: Arc::new(db_pool),
        }
    }
}
=== ./src/api/mod.rs ===
pub mod handlers;
pub mod routes;

pub use routes::configure_routes;

use crate::config::AppConfig;
use crate::database;
use reqwest::Client;
use sqlx::SqlitePool;

#[derive(Clone)]
pub struct AppState {
    pub config: AppConfig,
    pub client: Client,
    pub db_pool: Option<SqlitePool>,
}

impl AppState {
    pub async fn new(config: AppConfig) -> Self {
        let db_pool = match database::init_db().await {
            Ok(pool) => Some(pool),
            Err(e) => {
                log::error!("Failed to initialize database: {}", e);
                None
            }
        };
        Self { config, client: Client::new(), db_pool }
    }
}
