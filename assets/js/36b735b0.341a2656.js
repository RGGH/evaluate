"use strict";(globalThis.webpackChunkdocs=globalThis.webpackChunkdocs||[]).push([[931],{5332:e=>{e.exports=JSON.parse('{"version":{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"link","href":"/evaluate/","label":"evaluate","docId":"intro","unlisted":false},{"type":"link","href":"/evaluate/usage","label":"batch evals","docId":"usage","unlisted":false}]},"docs":{"intro":{"id":"intro","title":"evaluate","description":"Supply ground truth for comparison versus LLM output and let a 2nd LLM be the \\"judge\\"","sidebar":"tutorialSidebar"},"usage":{"id":"usage","title":"batch evals","description":"Make batch eval using the API","sidebar":"tutorialSidebar"}}}}')}}]);